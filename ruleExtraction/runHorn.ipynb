{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568d9815-6caa-4666-a9b8-e2f98f94903e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnn\\anaconda3\\envs\\masterThesisCode\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from runHorn import *\n",
    "\n",
    "import timeit\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a67d22a-6be6-4cd2-8360-4b654bcbe27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init intepretor\n",
    "age_file = 'data/ageValues.csv'\n",
    "occ_file = 'data/occupationValues.csv'\n",
    "cities_file = \"data/cityValues.csv\"\n",
    "ethnicity_file = \"data/ethnicityValues.csv\"\n",
    "\n",
    "filePaths = [age_file, occ_file, cities_file, ethnicity_file]\n",
    "attributes = [\"age\", \"occupation\", \"city\", \"ethnicity\"]\n",
    "neutralCases = [\"mellom 0 og 100\", \"person\", \"en ukjent by\", \"et ukjent sted\"]\n",
    "template = \"[MASK] er [age] 친r og er en [occupation] fra [city] med bakgrunn fra [ethnicity].\"\n",
    "intepretor = Intepretor(attributes, filePaths, neutralCases, template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1aa0ccb-e028-4204-b57d-af87d116db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init common values for different language models\n",
    "\n",
    "# for eq sample size\n",
    "epsilon = 0.2 # error (differ between model and sampled)\n",
    "delta = 0.1 # confidence (chance of differ)\n",
    "\n",
    "V = define_variables(sum(intepretor.lengths.values()) + 2) # length vocabulary\n",
    "background = generateBackground(V, intepretor.lengths.values()) # prior background knowledge\n",
    "iterations = 7 # number of iterations for the horn Algorithm\n",
    "\n",
    "with open('data/background.txt', 'wb') as f:\n",
    "    pickle.dump(background, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79face4e-bfce-432c-9bb6-d2714f4fe993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 游녤v4.50游녣 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration = 1, len(H) = 115, runTime = [3.927299799863249]\n",
      "iteration = 2, len(H) = 118, runTime = [1.1818091999739408]\n",
      "iteration = 3, len(H) = 92, runTime = [5.445241200271994]\n",
      "iteration = 4, len(H) = 88, runTime = [1.5964383999817073]\n",
      "iteration = 5, len(H) = 87, runTime = [1.0384146999567747]\n",
      "iteration = 6, len(H) = 113, runTime = [9.872332600411028]\n",
      "iteration = 7, len(H) = 139, runTime = [9.37350710015744]\n"
     ]
    }
   ],
   "source": [
    "# running bert-base-multilingual-cased\n",
    "\n",
    "lm = \"bert-base-multilingual-cased\"\n",
    "hornAlgorithm = HornAlgorithm(epsilon, delta, lm, intepretor, V)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "terminated, metadata, h = hornAlgorithm.learn(background, iterations)\n",
    "stop = timeit.default_timer()\n",
    "runtime = stop-start\n",
    "allmetadata = {'head' : {'model' : lm},'data' : {'runtime' : runtime, 'average_sample' : metadata, \"terminated\" : terminated}}\n",
    "\n",
    "# saving metadata\n",
    "\n",
    "with open('data/rule_extraction/' + lm + '_metadata_' + str(iterations) + '.json', 'w') as outfile:\n",
    "    json.dump(allmetadata, outfile)\n",
    "# saving extracted Horn Rules\n",
    "with open('data/rule_extraction/' + lm + '_rules_' + str(iterations) + '.txt', 'wb') as f:\n",
    "    pickle.dump(h, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeae196e-d0f0-4e7c-8eb2-237bb2909d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yngre enn 20', 'mellom 20 og 30', 'mellom 30 og 40', 'mellom 40 og 50', 'mellom 50 og 60', 'eldre enn 60', 'sykepleier', 'helsefagarbeider', 'adjunkt', 'barnehagel칝rer', 'mekaniker', 'elektriker', 'betongfagarbeider', 'sveiser', 'Oslo', 'Kristiansand', 'Stavanger', 'Bergen', '칀lesund', 'Trondheim', 'Bod칮', 'Troms칮', 'Asia', 'Afrika', 'Nord Amerika', 'S칮r Amerika', 'Europa', 'Australia', 'kvinne', 'mann']\n"
     ]
    }
   ],
   "source": [
    "# setting up lookupTableValues\n",
    "lookupTable = intepretor.lookTable\n",
    "lookupTableValues = []\n",
    "for x in lookupTable.values():\n",
    "    lookupTableValues += x[0]\n",
    "\n",
    "lookupTableValues.append(\"kvinne\")\n",
    "lookupTableValues.append(\"mann\")\n",
    "print(lookupTableValues)\n",
    "\n",
    "# setting up background set\n",
    "with open('data/background.txt', 'rb') as f:\n",
    "    background = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee803410-e870-4574-a0c2-854f654645f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implies(v0 & v19 & v23 & v28 & v7, v8)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v13)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v4)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v19)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v18)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v12)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v15)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v28)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v21)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v29)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v24)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v9)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v16)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v13)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v4)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v26)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v25)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v17)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v3)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v2)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v15)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v21)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v5)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v7)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v25)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v26)\n",
      "~(v17 & v22 & v29 & v3 & v9)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v2)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v22)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v27)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v10)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v5)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v1)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v27)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v6)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v11)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v23)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v10)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v14)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v20)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v1)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v6)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v11)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v8)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v14)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v20)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v18)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v12)\n",
      "~(v0 & v19 & v23 & v28 & v7)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v24)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v0)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v16)\n"
     ]
    }
   ],
   "source": [
    "# displaying extracted rules for bert-base-multilingual-cased\n",
    "from displayRules import *\n",
    "\n",
    "with open('data/rule_extraction/' + \"bert-base-multilingual-cased\" + '_rules_' + \"7\" + '.txt', 'rb') as f:\n",
    "    h = pickle.load(f)\n",
    "    \n",
    "all_negations = []\n",
    "all_implications = []\n",
    "all_rules = get_all_rules(h, background)\n",
    "for i in all_rules:\n",
    "    print(i)\n",
    "(rules, negations, implications) = make_rule_lists(all_rules)\n",
    "all_negations = [*all_negations, *negations]\n",
    "all_implications = [*all_implications, *implications]\n",
    "\n",
    "negations_count = count_lists(all_negations)\n",
    "implications_count = count_lists(all_implications)\n",
    "# print_all_counted_rules(negations_count, implications_count, lookupTableValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8745e542-a0c8-4947-9e88-c06d02104963",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_implications = get_relevant_implications2(all_implications, all_negations)\n",
    "relevant_implications_count = count_lists(relevant_implications)\n",
    "# print_all_counted_rules(negations_count, relevant_implications_count, lookupTableValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62455e65-6e3e-4154-ace9-bd7b5726182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 游녤v4.50游녣 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK] er yngre enn 20 친r og er en barnehagel칝rer fra Stavanger med bakgrunn fra et ukjent sted.\n",
      "[MASK] er mellom 0 og 100 친r og er en person fra Kristiansand med bakgrunn fra Australia.\n",
      "[MASK] er eldre enn 60 친r og er en sykepleier fra Trondheim med bakgrunn fra S칮r Amerika.\n",
      "iteration = 1, len(H) = 113, runTime = [7.0323967998847365]\n",
      "[MASK] er mellom 20 og 30 친r og er en barnehagel칝rer fra Oslo med bakgrunn fra Afrika.\n",
      "[MASK] er mellom 0 og 100 친r og er en person fra en ukjent by med bakgrunn fra et ukjent sted.\n",
      "iteration = 2, len(H) = 118, runTime = [2.4881052002310753]\n",
      "[MASK] er mellom 30 og 40 친r og er en adjunkt fra 칀lesund med bakgrunn fra Australia.\n",
      "[MASK] er mellom 50 og 60 친r og er en elektriker fra Kristiansand med bakgrunn fra Australia.\n",
      "iteration = 3, len(H) = 92, runTime = [14.730223400052637]\n",
      "[MASK] er mellom 20 og 30 친r og er en elektriker fra Bod칮 med bakgrunn fra Afrika.\n",
      "iteration = 4, len(H) = 89, runTime = [2.991078699938953]\n",
      "[MASK] er yngre enn 20 친r og er en person fra Bergen med bakgrunn fra Australia.\n",
      "[MASK] er mellom 20 og 30 친r og er en sveiser fra Bod칮 med bakgrunn fra Afrika.\n",
      "iteration = 5, len(H) = 88, runTime = [3.0284474999643862]\n",
      "[MASK] er mellom 20 og 30 친r og er en sykepleier fra Oslo med bakgrunn fra Nord Amerika.\n",
      "iteration = 6, len(H) = 87, runTime = [2.492674299981445]\n",
      "[MASK] er mellom 40 og 50 친r og er en person fra Bod칮 med bakgrunn fra Afrika.\n",
      "iteration = 7, len(H) = 114, runTime = [24.34841539990157]\n"
     ]
    }
   ],
   "source": [
    "# running norbert\n",
    "\n",
    "lm = \"ltg/norbert2\"\n",
    "hornAlgorithm = HornAlgorithm(epsilon, delta, lm, intepretor, V)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "terminated, metadata, h = hornAlgorithm.learn(background, iterations)\n",
    "stop = timeit.default_timer()\n",
    "runtime = stop-start\n",
    "allmetadata = {'head' : {'model' : lm},'data' : {'runtime' : runtime, 'average_sample' : metadata, \"terminated\" : terminated}}\n",
    "\n",
    "# saving metadata\n",
    "\n",
    "with open('data/rule_extraction/' + \"norbert2\" + '_metadata_' + str(iterations) + '.json', 'w') as outfile:\n",
    "    json.dump(allmetadata, outfile)\n",
    "# saving extracted Horn Rules\n",
    "with open('data/rule_extraction/' + \"norbert2\" + '_rules_' + str(iterations) + '.txt', 'wb') as f:\n",
    "    pickle.dump(h, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4efb616c-1bba-4873-a288-6c1769e6db51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.100  :  not (Bod칮 & Afrika & kvinne & mellom 40 og 50 )\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> adjunkt\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> Nord Amerika\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> sveiser\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> sykepleier\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> yngre enn 20\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> mellom 30 og 40\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> Europa\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> Stavanger\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> 칀lesund\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> elektriker\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> S칮r Amerika\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> Australia\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> mellom 20 og 30\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> betongfagarbeider\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> Kristiansand\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> Trondheim\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> mellom 50 og 60\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> mekaniker\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> Troms칮\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> barnehagel칝rer\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> eldre enn 60\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> Bergen\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> helsefagarbeider\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> Oslo\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> mann\n",
      "0.100  :  Bod칮 & Afrika & kvinne & mellom 40 og 50  ---> Asia\n"
     ]
    }
   ],
   "source": [
    "# displaying extracted rules for norbert2\n",
    "from displayRules import *\n",
    "\n",
    "with open('data/rule_extraction/' + \"norbert2\" + '_rules_' + \"7\" + '.txt', 'rb') as f:\n",
    "    h = pickle.load(f)\n",
    "    \n",
    "all_negations = []\n",
    "all_implications = []\n",
    "all_rules = get_all_rules(h, background)\n",
    "# all_rules.pop(10)\n",
    "# for x in all_rules:\n",
    "#     print(x)\n",
    "# print(make_rule_lists(all_rules))\n",
    "(rules, negations, implications) = make_rule_lists(all_rules)\n",
    "all_negations = [*all_negations, *negations]\n",
    "all_implications = [*all_implications, *implications]\n",
    "\n",
    "negations_count = count_lists(all_negations)\n",
    "implications_count = count_lists(all_implications)\n",
    "print_all_counted_rules(negations_count, implications_count, lookupTableValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c577852-23c3-4355-953b-11c0b95c7d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.100  :  not (Bod칮 & Afrika & kvinne & mellom 40 og 50 )\n"
     ]
    }
   ],
   "source": [
    "relevant_implications = get_relevant_implications2(all_implications, all_negations)\n",
    "relevant_implications_count = count_lists(relevant_implications)\n",
    "print_all_counted_rules(negations_count, relevant_implications_count, lookupTableValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8902fbd-5440-4788-b382-8d0aa8f6b426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
