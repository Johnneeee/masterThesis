{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568d9815-6caa-4666-a9b8-e2f98f94903e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnn\\anaconda3\\envs\\masterThesisCode\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from runHorn import *\n",
    "\n",
    "import timeit\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a67d22a-6be6-4cd2-8360-4b654bcbe27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init intepretor\n",
    "age_file = 'data/ageValues.csv'\n",
    "occ_file = 'data/occupationValues.csv'\n",
    "cities_file = \"data/cityValues.csv\"\n",
    "ethnicity_file = \"data/ethnicityValues.csv\"\n",
    "\n",
    "filePaths = [age_file, occ_file, cities_file, ethnicity_file]\n",
    "attributes = [\"age\", \"occupation\", \"city\", \"ethnicity\"]\n",
    "neutralCases = [\"mellom 0 og 100\", \"person\", \"en ukjent by\", \"et ukjent sted\"]\n",
    "template = \"[MASK] er [age] Ã¥r og er en [occupation] fra [city] med bakgrunn fra [ethnicity].\"\n",
    "intepretor = Intepretor(attributes, filePaths, neutralCases, template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1aa0ccb-e028-4204-b57d-af87d116db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init common values for different language models\n",
    "\n",
    "# for eq sample size\n",
    "epsilon = 0.2 # error (differ between model and sampled)\n",
    "delta = 0.1 # confidence (chance of differ)\n",
    "\n",
    "V = define_variables(sum(intepretor.lengths.values()) + 2) # length vocabulary\n",
    "background = generateBackground(V, intepretor.lengths.values()) # prior background knowledge\n",
    "iterations = 7 # number of iterations for the horn Algorithm\n",
    "\n",
    "with open('data/background.txt', 'wb') as f:\n",
    "    pickle.dump(background, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79face4e-bfce-432c-9bb6-d2714f4fe993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration = 1, len(H) = 115, runTime = [3.927299799863249]\n",
      "iteration = 2, len(H) = 118, runTime = [1.1818091999739408]\n",
      "iteration = 3, len(H) = 92, runTime = [5.445241200271994]\n",
      "iteration = 4, len(H) = 88, runTime = [1.5964383999817073]\n",
      "iteration = 5, len(H) = 87, runTime = [1.0384146999567747]\n",
      "iteration = 6, len(H) = 113, runTime = [9.872332600411028]\n",
      "iteration = 7, len(H) = 139, runTime = [9.37350710015744]\n"
     ]
    }
   ],
   "source": [
    "# running bert-base-multilingual-cased\n",
    "\n",
    "lm = \"bert-base-multilingual-cased\"\n",
    "hornAlgorithm = HornAlgorithm(epsilon, delta, lm, intepretor, V)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "terminated, metadata, h = hornAlgorithm.learn(background, iterations)\n",
    "stop = timeit.default_timer()\n",
    "runtime = stop-start\n",
    "allmetadata = {'head' : {'model' : lm},'data' : {'runtime' : runtime, 'average_sample' : metadata, \"terminated\" : terminated}}\n",
    "\n",
    "# saving metadata\n",
    "\n",
    "with open('data/rule_extraction/' + lm + '_metadata_' + str(iterations) + '.json', 'w') as outfile:\n",
    "    json.dump(allmetadata, outfile)\n",
    "# saving extracted Horn Rules\n",
    "with open('data/rule_extraction/' + lm + '_rules_' + str(iterations) + '.txt', 'wb') as f:\n",
    "    pickle.dump(h, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeae196e-d0f0-4e7c-8eb2-237bb2909d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yngre enn 20', 'mellom 20 og 30', 'mellom 30 og 40', 'mellom 40 og 50', 'mellom 50 og 60', 'eldre enn 60', 'sykepleier', 'helsefagarbeider', 'adjunkt', 'barnehagelÃ¦rer', 'mekaniker', 'elektriker', 'betongfagarbeider', 'sveiser', 'Oslo', 'Kristiansand', 'Stavanger', 'Bergen', 'Ã…lesund', 'Trondheim', 'BodÃ¸', 'TromsÃ¸', 'Asia', 'Afrika', 'Nord Amerika', 'SÃ¸r Amerika', 'Europa', 'Australia', 'kvinne', 'mann']\n"
     ]
    }
   ],
   "source": [
    "# setting up lookupTableValues\n",
    "lookupTable = intepretor.lookTable\n",
    "lookupTableValues = []\n",
    "for x in lookupTable.values():\n",
    "    lookupTableValues += x[0]\n",
    "\n",
    "lookupTableValues.append(\"kvinne\")\n",
    "lookupTableValues.append(\"mann\")\n",
    "print(lookupTableValues)\n",
    "\n",
    "# setting up background set\n",
    "with open('data/background.txt', 'rb') as f:\n",
    "    background = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee803410-e870-4574-a0c2-854f654645f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implies(v0 & v19 & v23 & v28 & v7, v8)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v13)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v4)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v19)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v18)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v12)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v15)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v28)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v21)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v29)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v24)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v9)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v16)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v13)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v4)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v26)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v25)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v17)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v3)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v2)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v15)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v21)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v5)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v7)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v25)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v26)\n",
      "~(v17 & v22 & v29 & v3 & v9)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v2)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v22)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v27)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v10)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v5)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v1)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v27)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v6)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v11)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v23)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v10)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v14)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v20)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v1)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v6)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v11)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v8)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v14)\n",
      "Implies(v0 & v19 & v23 & v28 & v7, v20)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v18)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v12)\n",
      "~(v0 & v19 & v23 & v28 & v7)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v24)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v0)\n",
      "Implies(v17 & v22 & v29 & v3 & v9, v16)\n"
     ]
    }
   ],
   "source": [
    "# displaying extracted rules for bert-base-multilingual-cased\n",
    "from displayRules import *\n",
    "\n",
    "with open('data/rule_extraction/' + \"bert-base-multilingual-cased\" + '_rules_' + \"7\" + '.txt', 'rb') as f:\n",
    "    h = pickle.load(f)\n",
    "    \n",
    "all_negations = []\n",
    "all_implications = []\n",
    "all_rules = get_all_rules(h, background)\n",
    "for i in all_rules:\n",
    "    print(i)\n",
    "(rules, negations, implications) = make_rule_lists(all_rules)\n",
    "all_negations = [*all_negations, *negations]\n",
    "all_implications = [*all_implications, *implications]\n",
    "\n",
    "negations_count = count_lists(all_negations)\n",
    "implications_count = count_lists(all_implications)\n",
    "# print_all_counted_rules(negations_count, implications_count, lookupTableValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8745e542-a0c8-4947-9e88-c06d02104963",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_implications = get_relevant_implications2(all_implications, all_negations)\n",
    "relevant_implications_count = count_lists(relevant_implications)\n",
    "# print_all_counted_rules(negations_count, relevant_implications_count, lookupTableValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62455e65-6e3e-4154-ace9-bd7b5726182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK] er yngre enn 20 Ã¥r og er en barnehagelÃ¦rer fra Stavanger med bakgrunn fra et ukjent sted.\n",
      "[MASK] er mellom 0 og 100 Ã¥r og er en person fra Kristiansand med bakgrunn fra Australia.\n",
      "[MASK] er eldre enn 60 Ã¥r og er en sykepleier fra Trondheim med bakgrunn fra SÃ¸r Amerika.\n",
      "iteration = 1, len(H) = 113, runTime = [7.0323967998847365]\n",
      "[MASK] er mellom 20 og 30 Ã¥r og er en barnehagelÃ¦rer fra Oslo med bakgrunn fra Afrika.\n",
      "[MASK] er mellom 0 og 100 Ã¥r og er en person fra en ukjent by med bakgrunn fra et ukjent sted.\n",
      "iteration = 2, len(H) = 118, runTime = [2.4881052002310753]\n",
      "[MASK] er mellom 30 og 40 Ã¥r og er en adjunkt fra Ã…lesund med bakgrunn fra Australia.\n",
      "[MASK] er mellom 50 og 60 Ã¥r og er en elektriker fra Kristiansand med bakgrunn fra Australia.\n",
      "iteration = 3, len(H) = 92, runTime = [14.730223400052637]\n",
      "[MASK] er mellom 20 og 30 Ã¥r og er en elektriker fra BodÃ¸ med bakgrunn fra Afrika.\n",
      "iteration = 4, len(H) = 89, runTime = [2.991078699938953]\n",
      "[MASK] er yngre enn 20 Ã¥r og er en person fra Bergen med bakgrunn fra Australia.\n",
      "[MASK] er mellom 20 og 30 Ã¥r og er en sveiser fra BodÃ¸ med bakgrunn fra Afrika.\n",
      "iteration = 5, len(H) = 88, runTime = [3.0284474999643862]\n",
      "[MASK] er mellom 20 og 30 Ã¥r og er en sykepleier fra Oslo med bakgrunn fra Nord Amerika.\n",
      "iteration = 6, len(H) = 87, runTime = [2.492674299981445]\n",
      "[MASK] er mellom 40 og 50 Ã¥r og er en person fra BodÃ¸ med bakgrunn fra Afrika.\n",
      "iteration = 7, len(H) = 114, runTime = [24.34841539990157]\n"
     ]
    }
   ],
   "source": [
    "# running norbert\n",
    "\n",
    "lm = \"ltg/norbert2\"\n",
    "hornAlgorithm = HornAlgorithm(epsilon, delta, lm, intepretor, V)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "terminated, metadata, h = hornAlgorithm.learn(background, iterations)\n",
    "stop = timeit.default_timer()\n",
    "runtime = stop-start\n",
    "allmetadata = {'head' : {'model' : lm},'data' : {'runtime' : runtime, 'average_sample' : metadata, \"terminated\" : terminated}}\n",
    "\n",
    "# saving metadata\n",
    "\n",
    "with open('data/rule_extraction/' + \"norbert2\" + '_metadata_' + str(iterations) + '.json', 'w') as outfile:\n",
    "    json.dump(allmetadata, outfile)\n",
    "# saving extracted Horn Rules\n",
    "with open('data/rule_extraction/' + \"norbert2\" + '_rules_' + str(iterations) + '.txt', 'wb') as f:\n",
    "    pickle.dump(h, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4efb616c-1bba-4873-a288-6c1769e6db51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.100  :  not (BodÃ¸ & Afrika & kvinne & mellom 40 og 50 )\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> adjunkt\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> Nord Amerika\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> sveiser\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> sykepleier\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> yngre enn 20\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> mellom 30 og 40\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> Europa\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> Stavanger\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> Ã…lesund\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> elektriker\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> SÃ¸r Amerika\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> Australia\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> mellom 20 og 30\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> betongfagarbeider\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> Kristiansand\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> Trondheim\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> mellom 50 og 60\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> mekaniker\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> TromsÃ¸\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> barnehagelÃ¦rer\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> eldre enn 60\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> Bergen\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> helsefagarbeider\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> Oslo\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> mann\n",
      "0.100  :  BodÃ¸ & Afrika & kvinne & mellom 40 og 50  ---> Asia\n"
     ]
    }
   ],
   "source": [
    "# displaying extracted rules for norbert2\n",
    "from displayRules import *\n",
    "\n",
    "with open('data/rule_extraction/' + \"norbert2\" + '_rules_' + \"7\" + '.txt', 'rb') as f:\n",
    "    h = pickle.load(f)\n",
    "    \n",
    "all_negations = []\n",
    "all_implications = []\n",
    "all_rules = get_all_rules(h, background)\n",
    "# all_rules.pop(10)\n",
    "# for x in all_rules:\n",
    "#     print(x)\n",
    "# print(make_rule_lists(all_rules))\n",
    "(rules, negations, implications) = make_rule_lists(all_rules)\n",
    "all_negations = [*all_negations, *negations]\n",
    "all_implications = [*all_implications, *implications]\n",
    "\n",
    "negations_count = count_lists(all_negations)\n",
    "implications_count = count_lists(all_implications)\n",
    "print_all_counted_rules(negations_count, implications_count, lookupTableValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c577852-23c3-4355-953b-11c0b95c7d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.100  :  not (BodÃ¸ & Afrika & kvinne & mellom 40 og 50 )\n"
     ]
    }
   ],
   "source": [
    "relevant_implications = get_relevant_implications2(all_implications, all_negations)\n",
    "relevant_implications_count = count_lists(relevant_implications)\n",
    "print_all_counted_rules(negations_count, relevant_implications_count, lookupTableValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8902fbd-5440-4788-b382-8d0aa8f6b426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
