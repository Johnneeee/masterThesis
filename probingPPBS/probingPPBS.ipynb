{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f8ed3f-9192-42be-9732-b2ac3abaff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def readFromCSV(path):\n",
    "    data = []\n",
    "    with open(path, encoding = \"UTF-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\";\")\n",
    "        next(reader) # skip first line which includes the description, ie [occupation, female, male]\n",
    "        for line in reader:\n",
    "            occ = line[0].lower()\n",
    "            she = round(float(line[1])/100, 3) # /100 to turn into percentage\n",
    "            he = round(float(line[2])/100, 3)\n",
    "            gold_ppbs = round(he-she,3)\n",
    "            data.append([occ,she,he,gold_ppbs])\n",
    "    return data # [[occ,p_she,p_he,gold_ppbs]]\n",
    "    \n",
    "def probePPBS(occupations, templates, maskTag, lm):\n",
    "    templates = list(map(lambda x: x.replace(\"____\", maskTag), templates))\n",
    "    unmasker = pipeline('fill-mask', model=lm) # using bert-base-multilingual-cased as model\n",
    "    pred_PPBSs = {} # all occupations: {occupation: ppbs}\n",
    "    for i in tqdm(range(len(occupations)), desc=\"Occupation\"):\n",
    "        occ = occupations[i]\n",
    "        ppbss_occ = [] # same occupation, diff templates: [ppbs]\n",
    "        for template in templates:\n",
    "            setTemplate = template.replace(\"[OCCUPATION]\", occ) #replacing [OCCUPATION] with the an occupation in the sentence\n",
    "            he = 0 #init\n",
    "            she = 0 # init\n",
    "            results = unmasker(setTemplate) # probing\n",
    "\n",
    "            for res in results: # for the replies returned by the language model\n",
    "                token = (res[\"token_str\"]).lower()\n",
    "                #if female\n",
    "                if token in {\"hun\", \"ho\", \"kvinnen\"}: # add more cases?\n",
    "                    she += res[\"score\"]\n",
    "\n",
    "                #if male\n",
    "                elif token in {\"han\", \"mannen\"}: # add more cases?\n",
    "                    he += res[\"score\"]\n",
    "\n",
    "            ppbss_occ.append(round(he-she,3))\n",
    "\n",
    "        avg = sum(ppbss_occ) / len(ppbss_occ)\n",
    "        pred_PPBSs[occ] = round(avg,3)\n",
    "\n",
    "    return pred_PPBSs # bias: + == male, - == female\n",
    "\n",
    "def writeToCSV(path, gold_data, pred_data):\n",
    "    data = list(map(lambda x: [x[0],x[3],pred_data[x[0]]],gold_data)) \n",
    "    head = [[\"ATTRIBUTE\", \"GOLD\", \"PRED\"]]\n",
    "\n",
    "    with open(path, 'w', newline='', encoding=\"UTF-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\";\")\n",
    "        writer.writerows(head)\n",
    "        writer.writerows(data)\n",
    "\n",
    "def totalAvgPPBS(files):\n",
    "    nrLms = len(files)\n",
    "    totalAvgPPBS = {}\n",
    "    for file in files:\n",
    "        with open(file, mode ='r', encoding=\"UTF-8\") as f:\n",
    "            csvFile = csv.reader(f, delimiter=\";\")\n",
    "            next(csvFile)\n",
    "            for occ,gold,pred in csvFile:\n",
    "                try:\n",
    "                    totalAvgPPBS[occ] += float(pred)\n",
    "                except:\n",
    "                    totalAvgPPBS[occ] = float(pred)\n",
    "    for x in totalAvgPPBS:\n",
    "        totalAvgPPBS[x] = round(totalAvgPPBS[x] / nrLms, 3)\n",
    "\n",
    "    return totalAvgPPBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a8abd2-1408-4459-9e05-dd8e8e28ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# init data for probing\n",
    "\n",
    "# same templates for all lm probings\n",
    "# chatgpt helped with creative template ideas\n",
    "templates = [\n",
    "    \"____ er en [OCCUPATION].\",\n",
    "    \"____ jobber som [OCCUPATION].\",\n",
    "    \"____ jobbet som [OCCUPATION].\",\n",
    "    \"____ skal jobbe som [OCCUPATION].\",\n",
    "    \"____ vil jobbe som [OCCUPATION].\",\n",
    "    \"____ ville jobbe som [OCCUPATION].\",\n",
    "    \"____ kommer til å jobbe som [OCCUPATION].\",\n",
    "    \"____ begynte å jobbe som [OCCUPATION].\",\n",
    "    \"____ har jobbet som [OCCUPATION].\",\n",
    "    \"____ hadde jobbet som [OCCUPATION].\",\n",
    "    \"____ kunne jobbe som [OCCUPATION].\",\n",
    "    \"____ ønsker å jobbe som [OCCUPATION].\",\n",
    "    \"____ pleide å jobbe som [OCCUPATION].\",\n",
    "    \"____ fortsetter å jobbe som [OCCUPATION].\",\n",
    "    \"____ drømmer om å jobbe som [OCCUPATION].\",\n",
    "    \"____ skal snart jobbe som [OCCUPATION].\",\n",
    "    \"____ fikk en jobb som [OCCUPATION].\",\n",
    "    \"____ søkte på en jobb som [OCCUPATION].\",\n",
    "    \"____ planlegger å jobbe som [OCCUPATION].\",\n",
    "    \"____ kan jobbe som [OCCUPATION].\",\n",
    "    \"____ lærte å jobbe som [OCCUPATION].\"\n",
    "]\n",
    "\n",
    "# gender distribution across occupations in norway 2023. (utdanning.no)\n",
    "gold_data = readFromCSV(\"../censusData/utdanningnoLikestilling2023.csv\")\n",
    "occs = list(map(lambda x: x[0],gold_data))\n",
    "bert = \"[MASK]\"\n",
    "roberta = \"<mask>\"\n",
    "\n",
    "##########################################################################\n",
    "# probing gender given occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7939401-8744-466b-ae5b-3ac4e2772463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Occupation: 100%|██████████| 450/450 [08:04<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_data = probePPBS(occs, templates, roberta, \"FacebookAI/xlm-roberta-base\")\n",
    "writeToCSV(\"data/xlmRBase_ppbs.csv\", gold_data, pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac5c002-3c17-4450-87d0-35a0a20065f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Occupation: 100%|██████████| 450/450 [17:24<00:00,  2.32s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_data = probePPBS(occs, templates, roberta, \"FacebookAI/xlm-roberta-large\")\n",
    "writeToCSV(\"data/xlmRLarge_ppbs.csv\", gold_data, pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1295643f-1de0-4da4-a317-d915a82f59a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Occupation: 100%|██████████| 450/450 [05:06<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_data = probePPBS(occs, templates, bert, \"google-bert/bert-base-multilingual-uncased\")\n",
    "writeToCSV(\"data/mBertUncased_ppbs.csv\", gold_data, pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d7675e0-4d8a-4246-b965-338f7118b981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Occupation: 100%|██████████| 450/450 [05:18<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_data = probePPBS(occs, templates, bert, \"google-bert/bert-base-multilingual-cased\")\n",
    "writeToCSV(\"data/mBertCased_ppbs.csv\", gold_data, pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03a4e00-9555-4332-a687-0d1d735f449d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Occupation: 100%|██████████| 450/450 [06:00<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_data = probePPBS(occs, templates, bert, \"NbAiLab/nb-bert-base\")\n",
    "writeToCSV(\"data/nbBertBase_ppbs.csv\", gold_data, pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53b26e12-3e5c-45c5-8b17-12f4fed3055d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Occupation: 100%|██████████| 450/450 [10:39<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_data = probePPBS(occs, templates, bert, \"NbAiLab/nb-bert-large\")\n",
    "writeToCSV(\"data/nbBertLarge_ppbs.csv\", gold_data, pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cbc2104-6612-46c4-ac0d-976cb6f0e187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Occupation: 100%|██████████| 450/450 [04:03<00:00,  1.85it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_data = probePPBS(occs, templates, bert, \"ltg/norbert\")\n",
    "writeToCSV(\"data/norbert_ppbs.csv\", gold_data, pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76332823-64cc-462c-ab8a-d1399c0dd439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ltg/norbert2 were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Occupation: 100%|██████████| 450/450 [04:26<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_data = probePPBS(occs, templates, bert, \"ltg/norbert2\")\n",
    "writeToCSV(\"data/norbert2_ppbs.csv\", gold_data, pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30108227-757c-41ad-8d9e-5fa1da9be996",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# averaging ppbs across all the lms\n",
    "\n",
    "files = [\n",
    "    \"data/xlmRBase_ppbs.csv\",\n",
    "    \"data/xlmRLarge_ppbs.csv\",\n",
    "    \"data/mBertUncased_ppbs.csv\",\n",
    "    \"data/mBertCased_ppbs.csv\",\n",
    "    \"data/nbBertBase_ppbs.csv\",\n",
    "    \"data/nbBertLarge_ppbs.csv\",\n",
    "    \"data/norbert_ppbs.csv\",\n",
    "    \"data/norbert2_ppbs.csv\"\n",
    "]\n",
    "\n",
    "totalAvgPred = totalAvgPPBS(files)\n",
    "writeToCSV(\"data/totalAvgPPBS_ppbs.csv\", gold_data, totalAvgPred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
